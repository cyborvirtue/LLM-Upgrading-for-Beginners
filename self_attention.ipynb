{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 AI 相关的面试中，经常会有面试官让写 self-attention，但是因为 transformer 这篇文章其实包含很多的细节，因此可能面试官对于 self-attention 实现到什么程度是有不同的预期。因此这里想通过写不同版本的 self-attention 实现来达到不同面试官的预期。以此告诉面试官，了解细节，但是处于时间考虑，可能只写了简化版本，如果有时间可以把完整的写出来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自注意力(Self-Attention)机制理论\n",
    "\n",
    "自注意力机制是Transformer架构的核心组成部分，它允许模型在处理序列数据时捕捉长距离依赖关系。与传统的RNN和CNN不同，自注意力机制可以直接建立序列中任意两个位置之间的联系，而不受位置距离的限制。\n",
    "\n",
    "## 基本原理\n",
    "\n",
    "自注意力的核心思想是：对于序列中的每个元素，通过计算它与序列中所有元素（包括自身）的关联程度，生成一个加权的表示。这种机制使模型能够\"关注\"序列中的相关部分，无论它们在序列中的位置如何。\n",
    "\n",
    "## 数学定义\n",
    "\n",
    "设输入序列表示为矩阵 $X \\in \\mathbb{R}^{n \\times d}$，其中 $n$ 是序列长度，$d$ 是特征维度。自注意力机制的计算过程如下：\n",
    "\n",
    "1. **线性投影**：首先，将输入 $X$ 投影到三个不同的空间，生成查询(Query)、键(Key)和值(Value)矩阵：\n",
    "   $$Q = XW^Q, \\quad K = XW^K, \\quad V = XW^V$$\n",
    "   \n",
    "   其中 $W^Q, W^K, W^V \\in \\mathbb{R}^{d \\times d_k}$ 是可学习的参数矩阵，$d_k$ 是投影后的维度。\n",
    "\n",
    "2. **注意力分数计算**：计算查询和键之间的相似度得到注意力分数矩阵：\n",
    "   $$S = QK^T = XW^Q(XW^K)^T \\in \\mathbb{R}^{n \\times n}$$\n",
    "   \n",
    "   矩阵 $S$ 中的每个元素 $S_{ij}$ 表示位置 $i$ 对位置 $j$ 的注意力分数。\n",
    "\n",
    "3. **缩放**：为了防止梯度消失问题，对注意力分数进行缩放：\n",
    "   $$S_{scaled} = \\frac{S}{\\sqrt{d_k}} = \\frac{QK^T}{\\sqrt{d_k}}$$\n",
    "   \n",
    "   这里的 $\\sqrt{d_k}$ 是缩放因子，$d_k$ 是键向量的维度。\n",
    "\n",
    "4. **掩码(可选)**：在某些情况下（如解码器中的自回归生成），需要防止位置 $i$ 获取到位置 $j > i$ 的信息，此时可以应用掩码：\n",
    "   $$S_{masked} = S_{scaled} \\odot M$$\n",
    "   \n",
    "   其中 $M \\in \\mathbb{R}^{n \\times n}$ 是掩码矩阵，$\\odot$ 表示元素乘法。\n",
    "\n",
    "5. **Softmax归一化**：对缩放后的注意力分数应用softmax函数，将其转换为概率分布：\n",
    "   $$A = \\text{softmax}(S_{scaled}) = \\frac{\\exp(S_{scaled})}{\\sum \\exp(S_{scaled})}$$\n",
    "   \n",
    "   其中，softmax按行应用，确保每行的和为1。\n",
    "\n",
    "6. **加权求和**：使用注意力权重对值矩阵进行加权求和，得到最终的输出：\n",
    "   $$O = AV = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "综合起来，自注意力机制可以表示为以下公式：\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "## 多头注意力(Multi-Head Attention)\n",
    "\n",
    "为了增强模型的表示能力，Transformer使用了多头注意力机制，即同时学习多组不同的线性投影：\n",
    "\n",
    "1. 对于第 $i$ 个头，计算：\n",
    "   $$Q_i = XW_i^Q, \\quad K_i = XW_i^K, \\quad V_i = XW_i^V$$\n",
    "   $$\\text{head}_i = \\text{Attention}(Q_i, K_i, V_i)$$\n",
    "\n",
    "2. 将所有头的输出拼接并通过一个线性变换：\n",
    "   $$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O$$\n",
    "\n",
    "其中 $h$ 是头的数量，$W^O \\in \\mathbb{R}^{hd_k \\times d}$ 是输出投影矩阵。\n",
    "\n",
    "多头注意力允许模型同时关注来自不同表示子空间的信息，丰富了表示能力。\n",
    "\n",
    "## 计算复杂度\n",
    "\n",
    "自注意力机制的时间复杂度为 $O(n^2 \\cdot d)$，其中 $n$ 是序列长度，$d$ 是特征维度。空间复杂度同样为 $O(n^2 \\cdot d)$。这意味着对于非常长的序列，计算成本会变得很高，这也是后续各种改进版本自注意力机制的主要优化方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面代码是参考b站视频的从零开始编写注意力机制的代码：\n",
    "b站视频：https://www.bilibili.com/video/BV19YbFeHETz/?spm_id_from=333.1387.homepage.video_card.click&vd_source=01e76602bca90a928935ecc928b2c476"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 写法1：简单写法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0629, 0.5470, 0.6495, 0.3939],\n",
      "         [0.2147, 0.9538, 0.2387, 0.8803]],\n",
      "\n",
      "        [[0.4591, 0.1322, 0.7582, 0.1914],\n",
      "         [0.1903, 0.4623, 0.6657, 0.3359]],\n",
      "\n",
      "        [[0.7230, 0.8849, 0.8254, 0.7832],\n",
      "         [0.5898, 0.4862, 0.6398, 0.1634]]])\n",
      "注意力分数： tensor([[[-0.1867, -0.2050],\n",
      "         [-0.2925, -0.3349]],\n",
      "\n",
      "        [[-0.0601, -0.0154],\n",
      "         [-0.1062, -0.1205]],\n",
      "\n",
      "        [[-0.0906, -0.0132],\n",
      "         [-0.0093,  0.0235]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9080,  0.1165,  0.0589, -0.3707],\n",
       "         [-0.9070,  0.1157,  0.0595, -0.3704]],\n",
       "\n",
       "        [[-0.7746, -0.1408,  0.1270, -0.2042],\n",
       "         [-0.7747, -0.1419,  0.1266, -0.2033]],\n",
       "\n",
       "        [[-1.0428, -0.0289,  0.0970, -0.1738],\n",
       "         [-1.0449, -0.0283,  0.0962, -0.1743]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention1(nn.Module):\n",
    "    def __init__(self, hidden_dim:int = 728):\n",
    "        super().__init__()\n",
    "        self.hidden_dim=hidden_dim\n",
    "        #初始化q,k,v的投影矩阵\n",
    "        self.query_proj=nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.key_proj=nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.value_proj=nn.Linear(hidden_dim,hidden_dim)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        #x: (batch_size, seq_len, hidden_dim)\n",
    "        Q=self.query_proj(x)\n",
    "        K=self.key_proj(x)\n",
    "        V=self.value_proj(x)\n",
    "        #attenion_value: (batch_size, seq_len, seq_len)\n",
    "        attention_value=torch.matmul(Q,K.transpose(-1,-2) )\n",
    "        print(\"注意力分数：\",attention_value)\n",
    "        attention_weight=torch.softmax(attention_value/math.sqrt(self.hidden_dim),dim=-1)\n",
    "        #output: (batch_size, seq_len, hidden_dim)\n",
    "        output=torch.matmul(attention_weight,V)\n",
    "        return output\n",
    "\n",
    "        \n",
    "X=torch.rand(3,2,4)\n",
    "print(X)\n",
    "net=SelfAttention1(4) #输入参数为init时候的hidden_dim\n",
    "net(X)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 代码实现过程中的补充知识点：\n",
    "#### 1.三维张量的矩阵乘法规则\n",
    "\n",
    "在深度学习框架（如PyTorch、TensorFlow）中，当对形状为 (B, M, N) 和 (B, N, P) 的两个三维张量进行矩阵乘法时：第一个维度（批次维度B）被视为独立的批次。对每个批次，执行普通的矩阵乘法（按照二维矩阵乘法规则），结果形状为 (B, M, P)。\n",
    "\n",
    "能够进行矩阵乘法的关键因素是遵循矩阵乘法的维度对齐规则，具体来说，对于两个矩阵 A 和 B 的乘法 A @ B：\n",
    "- A 的列数必须等于 B 的行数\n",
    "- 结果矩阵的形状将是 (A的行数, B的列数)\n",
    "\n",
    "对于批量矩阵乘法（如三维张量 A 和 B 的乘法）：\n",
    "1. **批次维度必须匹配或可广播**：两个张量的第一个维度（批次维度）必须相同，或者其中一个是1（可以广播）\n",
    "2. **内部维度必须匹配**：A 的最后一个维度必须等于 B 的倒数第二个维度\n",
    "3. **结果形状**：(批次大小, A的倒数第二个维度, B的最后一个维度)\n",
    "\n",
    "\n",
    "\n",
    "在你的例子中：\n",
    "\n",
    "第一次乘法：\n",
    "- Q: (batch_size, seq_len, hidden_dim)\n",
    "- K.transpose: (batch_size, hidden_dim, seq_len)\n",
    "- 对齐点：Q 的最后一个维度 (hidden_dim) = K.transpose 的倒数第二个维度 (hidden_dim)\n",
    "- 结果：(batch_size, seq_len, seq_len)\n",
    "\n",
    "第二次乘法：\n",
    "- attention_weight: (batch_size, seq_len, seq_len)\n",
    "- V: (batch_size, seq_len, hidden_dim)\n",
    "- 对齐点：attention_weight 的最后一个维度 (seq_len) = V 的倒数第二个维度 (seq_len)\n",
    "- 结果：(batch_size, seq_len, hidden_dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 写法2 合并计算\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2709, 0.3968, 0.7889, 0.8579],\n",
      "         [0.8249, 0.4581, 0.3383, 0.5962]],\n",
      "\n",
      "        [[0.2547, 0.4830, 0.6664, 0.0682],\n",
      "         [0.5068, 0.5681, 0.5868, 0.4327]],\n",
      "\n",
      "        [[0.0549, 0.2919, 0.9394, 0.2116],\n",
      "         [0.3851, 0.8969, 0.9019, 0.8795]]])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'prin' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(X)\n\u001b[1;32m     27\u001b[0m net\u001b[38;5;241m=\u001b[39mSelfAttention2(\u001b[38;5;241m4\u001b[39m) \u001b[38;5;66;03m#输入参数为init时候的hidden_dim\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM_shouxie/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM_shouxie/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[9], line 19\u001b[0m, in \u001b[0;36mSelfAttention2.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#dim=-1表示在最后一个维度上做softmax，也就是在最后一个维度上做归一化\u001b[39;00m\n\u001b[1;32m     18\u001b[0m attention_weight\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39msoftmax(torch\u001b[38;5;241m.\u001b[39mmatmul(Q,K\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m/\u001b[39mmath\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim)),dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mprin\u001b[49m\n\u001b[1;32m     20\u001b[0m output\u001b[38;5;241m=\u001b[39mattention_weight\u001b[38;5;129m@V\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prin' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class SelfAttention2(nn.Module):\n",
    "    def __init__(self, dim:int=728):\n",
    "        super().__init__()\n",
    "        self.dim=dim\n",
    "        #KV 矩阵计算的时候，可以合并成一个大矩阵计算。\n",
    "        self.qkv=nn.Linear(dim, dim*3)\n",
    "        self.output=nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        QKV=self.qkv(x)\n",
    "        #把QKV拆分成Q,K,V\n",
    "        Q,K,V=torch.split(QKV,self.dim,dim=-1)\n",
    "        #dim=-1表示在最后一个维度上做softmax，也就是在最后一个维度上做归一化\n",
    "        attention_weight=torch.softmax(torch.matmul(Q,K.transpose(-1,-2)/math.sqrt(self.dim)),dim=-1)\n",
    "        prin\n",
    "        output=attention_weight@V\n",
    "        return output\n",
    "        \n",
    "\n",
    "    \n",
    "X=torch.rand(3,2,4)\n",
    "print(X)\n",
    "net=SelfAttention2(4) #输入参数为init时候的hidden_dim\n",
    "net(X)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 写法3：加入dropout和attetion mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class SelfAttention3(nn.Module):\n",
    "    def __init__(self, dim,dropout=0.1,*args,**kwargs)->None:\n",
    "        super().__init__(*args,**kwargs)\n",
    "        self.dim=dim\n",
    "        self.attention_dropout=nn.Droupout(droupout)\n",
    "        self.output=nn.Linear(dim,dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一些对attetion的深入理解\n",
    "\n",
    "\n",
    "注意力分数是自注意力机制中衡量序列中不同位置之间关联强度的数值。它是通过查询向量(Query)和键向量(Key)的点积计算得到的中间结果。\n",
    "\n",
    "#### 数学表示\n",
    "\n",
    "在自注意力计算流程中，注意力分数的计算公式为：\n",
    "\n",
    "$$S = QK^T = XW^Q(XW^K)^T$$\n",
    "\n",
    "其中，$S$是一个$n \\times n$的矩阵，$n$是序列长度。矩阵$S$中的每个元素$S_{ij}$代表位置$i$对位置$j$的注意力分数。\n",
    "\n",
    "#### 注意力分数的含义\n",
    "\n",
    "注意力分数从本质上代表了**\"当前位置应该关注序列中哪些其他位置\"**的重要性权重。具体来说：\n",
    "\n",
    "1. **高分数**：表示两个位置之间有很强的语义关联，当前位置应该高度关注该位置的信息\n",
    "2. **低分数**：表示两个位置关联度低，当前位置可以忽略该位置的信息\n",
    "\n",
    "#### 直观理解\n",
    "\n",
    "可以从不同角度理解注意力分数：\n",
    "\n",
    "1. **信息检索视角**：\n",
    "   - Query是\"搜索查询\"\n",
    "   - Key是\"文档关键字\"\n",
    "   - 注意力分数反映了查询与各个关键字的匹配程度\n",
    "\n",
    "2. **语言理解视角**：\n",
    "   - 在自然语言中，注意力分数可以捕捉：\n",
    "     - 代词与其指代对象的关系\n",
    "     - 依存关系\n",
    "     - 语义上相关的词汇\n",
    "\n",
    "3. **特征集成视角**：\n",
    "   - 每个位置都在\"询问\"：序列中哪些位置包含与我相关的信息？\n",
    "   - 注意力分数给出了回答：这些位置的信息与你相关的程度\n",
    "\n",
    "#### 注意力分数的使用流程\n",
    "\n",
    "1. **计算原始分数**：$S = QK^T$\n",
    "2. **缩放**：$S_{scaled} = \\frac{S}{\\sqrt{d_k}}$（防止维度较高时梯度消失）\n",
    "3. **应用掩码**(如需)：屏蔽不应关注的位置（如填充位置或未来信息）\n",
    "4. **Softmax归一化**：$A = \\text{softmax}(S_{scaled})$，转换为概率分布\n",
    "5. **加权汇总**：用这些概率对值向量进行加权求和 $O = AV$\n",
    "\n",
    "#### 示例\n",
    "\n",
    "假设在一个句子\"猫坐在垫子上，它很舒适\"中：\n",
    "\n",
    "- 分析\"它\"这个位置时，注意力分数会对\"猫\"给予很高的权重，表明这两个位置高度相关\n",
    "- 而对\"垫子\"、\"上\"等词给予较低的权重\n",
    "- 这个机制使模型能够\"理解\"代词\"它\"指代的是\"猫\"\n",
    "\n",
    "通过注意力分数机制，Transformer系列模型能够有效捕捉序列数据中的长距离依赖关系，这是它们成功的关键因素之一。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_shouxie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
